"""Filesystem storage utilities for import/export workflow.
导入/导出工作流的文件系统存储工具。



This module is responsible for (该模块负责):
    - Creating import/export directories (imports/exports).
      创建导入/导出目录结构。
    - Persisting import metadata (`meta.json`).
      写入/读取导入元信息（meta.json）。
    - Storing parsed/validated parquet files.
      保存解析/校验后的 parquet 数据。
    - Cleaning up expired import workspaces.
      清理过期的导入工作目录。

The storage layout is fully configurable through `ImportExportConfig` or
`resolve_config(...)`.

存储布局可通过 `ImportExportConfig` 或 `resolve_config(...)` 完整配置。
"""

import hashlib
import json
import os
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any
from uuid import UUID

import uuid6

from fastapi_import_export.config import ImportExportConfig, resolve_config


@dataclass(frozen=True, slots=True)
class ImportPaths:
    """Resolved paths for a single import job.

    导入任务的文件系统路径。

    Attributes:
        root: Root directory for this import job.
            本次导入任务的根目录。
        original: Path prefix for uploaded original file (suffix appended later).
            原始上传文件路径前缀（后续会加上扩展名）。
        meta: Path to meta.json.
            meta.json 路径。
        parsed_parquet: Parsed full dataset parquet path.
            解析后的全量数据 parquet 路径。
        errors_json: Validation errors json path.
            校验错误 errors.json 路径。
        valid_parquet: Valid rows parquet path.
            通过校验的有效行 parquet 路径。
    """

    root: Path
    original: Path
    meta: Path
    parsed_parquet: Path
    errors_json: Path
    valid_parquet: Path


def new_import_id() -> UUID:
    """Create a new import job id.

    创建新的导入任务 ID。

    Returns:
        A UUID (UUIDv7 style generated by uuid6).
            导入任务 ID（uuid6 生成的 UUIDv7 风格）。
    """
    return uuid6.uuid7()


def now_ts() -> int:
    """Return current unix timestamp in seconds.

    返回当前 Unix 时间戳（秒）。

    Returns:
        Current timestamp in seconds.
            当前时间戳（秒）。
    """
    return int(time.time())


def ensure_dirs(*, config: ImportExportConfig) -> None:
    """Ensure imports/exports directories exist.

    确保 imports/exports 目录存在。

    Args:
        config: ImportExportConfig instance.
            导入导出配置。
    """
    config.imports_dir.mkdir(parents=True, exist_ok=True)
    config.exports_dir.mkdir(parents=True, exist_ok=True)


def get_import_paths(
    import_id: UUID, *, config: ImportExportConfig | None = None, base_dir: str | os.PathLike[str] | None = None
) -> ImportPaths:
    """Resolve all filesystem paths for a given import_id.

    为给定的导入任务 ID 解析所有文件系统路径。

    Args:
        import_id: Import job identifier.
            导入任务 ID。
        config: Optional pre-resolved config.
            可选：已解析好的配置。
        base_dir: Optional base directory override (used when config is not provided).
            可选：base_dir 覆盖（当 config 未提供时使用）。

    Returns:
        ImportPaths.
            返回 ImportPaths。

    Examples:
        >>> from uuid import UUID
        >>> from fastapi_import_export.storage import get_import_paths
        >>> p = get_import_paths(UUID("00000000-0000-0000-0000-000000000000"))
        >>> p.meta.name
        'meta.json'
    """
    cfg = config or resolve_config(base_dir=base_dir)
    root = cfg.imports_dir / str(import_id)
    return ImportPaths(
        root=root,
        original=root / "original",
        meta=root / "meta.json",
        parsed_parquet=root / "parsed.parquet",
        errors_json=root / "errors.json",
        valid_parquet=root / "valid.parquet",
    )


def write_meta(paths: ImportPaths, meta: dict[str, Any]) -> None:
    """Write meta.json for an import job.

    为导入任务写入 meta.json。

    Args:
        paths: ImportPaths.
            导入路径集合。
        meta: JSON-serializable metadata dict.
            可 JSON 序列化的元信息字典。
    """
    paths.root.mkdir(parents=True, exist_ok=True)
    paths.meta.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")


def read_meta(paths: ImportPaths) -> dict[str, Any]:
    """Read meta.json for an import job.

    读取导入任务的 meta.json。

    Args:
        paths: ImportPaths.
            导入路径集合。

    Returns:
        Parsed metadata dict.
            解析后的元信息字典。
    """
    return json.loads(paths.meta.read_text(encoding="utf-8"))


def sha256_file(file_path: Path) -> str:
    """Compute sha256 checksum of a file.

    计算文件的 sha256 校验和。

    Args:
        file_path: Path to the file.
            文件路径。

    Returns:
        Hex string sha256 digest.
            sha256 十六进制摘要字符串。
    """
    h = hashlib.sha256()
    with file_path.open("rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()


def safe_unlink(path: Path) -> None:
    """Best-effort unlink (ignore errors).

    尽力删除文件（忽略错误）。

    Args:
        path: File path.
            文件路径。
    """
    try:
        path.unlink(missing_ok=True)
    except Exception:
        pass


def safe_rmtree(path: Path) -> None:
    """Best-effort recursive delete for a directory (ignore errors).

    尽力递归删除目录（忽略错误）。

    Args:
        path: Directory path.
            目录路径。
    """
    try:
        if not path.exists():
            return
        for p in sorted(path.rglob("*"), reverse=True):
            if p.is_file():
                safe_unlink(p)
            else:
                try:
                    p.rmdir()
                except Exception:
                    pass
        try:
            path.rmdir()
        except Exception:
            pass
    except Exception:
        pass


def delete_export_file(path: str) -> None:
    """Helper for FastAPI BackgroundTask to delete an exported file.

    为 FastAPI BackgroundTask 提供的辅助函数，用于删除导出文件。

    Args:
        path: File path string.
            文件路径字符串。

    Examples:
        >>> from starlette.background import BackgroundTask
        >>> from fastapi_import_export.storage import delete_export_file
        >>> _ = BackgroundTask(delete_export_file, "export.csv")
    """
    safe_unlink(Path(path))


def create_export_path(
    filename: str,
    *,
    config: ImportExportConfig | None = None,
    base_dir: str | os.PathLike[str] | None = None,
) -> Path:
    """Create a safe path for an export file under exports directory.

    创建 exports 目录下的安全导出文件路径。

    Args:
        filename: Desired filename (will be sanitized).
            期望的文件名（会做简单清理，避免路径穿越）。
        config: Optional config override.
            可选：配置覆盖。
        base_dir: Optional base_dir override when config is not provided.
            可选：base_dir 覆盖（当 config 未提供时使用）。

    Returns:
        Path under exports directory.
            exports 目录下的安全路径。
    """
    cfg = config or resolve_config(base_dir=base_dir)
    ensure_dirs(config=cfg)
    safe_name = filename.replace("/", "_").replace("\\", "_")
    return cfg.exports_dir / safe_name


def cleanup_expired_imports(
    *,
    ttl_hours: int,
    config: ImportExportConfig | None = None,
    base_dir: str | os.PathLike[str] | None = None,
) -> int:
    """Cleanup expired import job directories.

    清理过期的导入任务目录。

    The function uses `meta.json.created_at` (unix timestamp) as the primary
    signal to decide expiration.

    函数使用 `meta.json.created_at`（Unix 时间戳）作为主要信号来判断过期。

    Args:
        ttl_hours: TTL in hours.
            过期时间（小时）。
        config: Optional config override.
            可选：配置覆盖。
        base_dir: Optional base_dir override when config is not provided.
            可选：base_dir 覆盖（当 config 未提供时使用）。

    Returns:
        Number of directories cleaned.
            清理的目录数量。
    """
    cfg = config or resolve_config(base_dir=base_dir)
    imports_dir = cfg.imports_dir
    if not imports_dir.exists():
        return 0
    cutoff = now_ts() - int(ttl_hours) * 3600
    cleaned = 0
    for item in imports_dir.iterdir():
        if not item.is_dir():
            continue
        meta_path = item / "meta.json"
        created_at = 0
        try:
            if meta_path.exists():
                meta = json.loads(meta_path.read_text(encoding="utf-8"))
                created_at = int(meta.get("created_at") or 0)
        except Exception:
            created_at = 0
        try:
            if created_at and created_at >= cutoff:
                continue
            safe_rmtree(item)
            cleaned += 1
        except Exception:
            continue
    return cleaned
